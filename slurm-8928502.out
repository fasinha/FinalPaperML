04/13/2020 14:15:06 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
04/13/2020 14:15:06 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/fs45/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
04/13/2020 14:15:06 - INFO - transformers.configuration_utils -   Model config BertConfig {
  "_num_labels": 2,
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": null,
  "do_sample": false,
  "early_stopping": false,
  "eos_token_ids": null,
  "finetuning_task": "blimptask",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_beams": 1,
  "num_hidden_layers": 12,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": null,
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

04/13/2020 14:15:06 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/fs45/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
04/13/2020 14:15:06 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/fs45/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
04/13/2020 14:15:10 - INFO - transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
04/13/2020 14:15:10 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
04/13/2020 14:15:14 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='/scratch/fs45/nlu/data/blimp/data/blimptask', device=device(type='cuda'), do_eval=True, do_lower_case=False, do_train=False, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=2e-05, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='bert-base-uncased', model_type='bert', n_gpu=1, no_cuda=False, num_train_epochs=3.0, output_dir='/scratch/fs45/nlu/blimptask_run/', output_mode='classification', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, save_steps=500, seed=42, server_ip='', server_port='', task_name='blimptask', tokenizer_name='', warmup_steps=0, weight_decay=0.0)
04/13/2020 14:15:14 - INFO - transformers.tokenization_utils -   Model name '/scratch/fs45/nlu/blimptask_run/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '/scratch/fs45/nlu/blimptask_run/' is a path, a model identifier, or url to a directory containing tokenizer files.
04/13/2020 14:15:14 - INFO - transformers.tokenization_utils -   Didn't find file /scratch/fs45/nlu/blimptask_run/added_tokens.json. We won't load it.
04/13/2020 14:15:14 - INFO - transformers.tokenization_utils -   loading file /scratch/fs45/nlu/blimptask_run/vocab.txt
04/13/2020 14:15:14 - INFO - transformers.tokenization_utils -   loading file None
04/13/2020 14:15:14 - INFO - transformers.tokenization_utils -   loading file /scratch/fs45/nlu/blimptask_run/special_tokens_map.json
04/13/2020 14:15:14 - INFO - transformers.tokenization_utils -   loading file /scratch/fs45/nlu/blimptask_run/tokenizer_config.json
04/13/2020 14:15:14 - INFO - __main__ -   Evaluate the following checkpoints: ['/scratch/fs45/nlu/blimptask_run/']
04/13/2020 14:15:14 - INFO - transformers.configuration_utils -   loading configuration file /scratch/fs45/nlu/blimptask_run/config.json
04/13/2020 14:15:14 - INFO - transformers.configuration_utils -   Model config BertConfig {
  "_num_labels": 2,
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": null,
  "do_sample": false,
  "early_stopping": false,
  "eos_token_ids": null,
  "finetuning_task": "blimptask",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_beams": 1,
  "num_hidden_layers": 12,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": null,
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

04/13/2020 14:15:14 - INFO - transformers.modeling_utils -   loading weights file /scratch/fs45/nlu/blimptask_run/pytorch_model.bin
04/13/2020 14:15:17 - INFO - __main__ -   Loading features from cached file /scratch/fs45/nlu/data/blimp/data/blimptask/cached_dev_bert-base-uncased_128_blimptask
04/13/2020 14:15:17 - INFO - __main__ -   ***** Running evaluation  *****
04/13/2020 14:15:17 - INFO - __main__ -     Num examples = 2000
04/13/2020 14:15:17 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/250 [00:00<?, ?it/s]Evaluating:   0%|          | 1/250 [00:00<01:18,  3.18it/s]Evaluating:   2%|▏         | 4/250 [00:00<00:56,  4.33it/s]Evaluating:   3%|▎         | 8/250 [00:00<00:41,  5.84it/s]Evaluating:   5%|▍         | 12/250 [00:00<00:30,  7.75it/s]Evaluating:   6%|▋         | 16/250 [00:00<00:23, 10.11it/s]Evaluating:   8%|▊         | 20/250 [00:00<00:17, 12.88it/s]Evaluating:  10%|▉         | 24/250 [00:01<00:14, 15.97it/s]Evaluating:  11%|█         | 28/250 [00:01<00:11, 19.18it/s]Evaluating:  13%|█▎        | 32/250 [00:01<00:09, 22.32it/s]Evaluating:  14%|█▍        | 36/250 [00:01<00:08, 24.95it/s]Evaluating:  16%|█▌        | 40/250 [00:01<00:07, 27.43it/s]Evaluating:  18%|█▊        | 44/250 [00:01<00:06, 29.44it/s]Evaluating:  19%|█▉        | 48/250 [00:01<00:06, 31.34it/s]Evaluating:  21%|██        | 52/250 [00:01<00:06, 32.39it/s]Evaluating:  22%|██▏       | 56/250 [00:01<00:05, 33.33it/s]Evaluating:  24%|██▍       | 60/250 [00:02<00:05, 34.14it/s]Evaluating:  26%|██▌       | 64/250 [00:02<00:05, 34.01it/s]Evaluating:  27%|██▋       | 68/250 [00:02<00:05, 34.75it/s]Evaluating:  29%|██▉       | 72/250 [00:02<00:05, 35.23it/s]Evaluating:  30%|███       | 76/250 [00:02<00:04, 35.54it/s]Evaluating:  32%|███▏      | 80/250 [00:02<00:04, 35.63it/s]Evaluating:  34%|███▎      | 84/250 [00:02<00:04, 35.24it/s]Evaluating:  35%|███▌      | 88/250 [00:02<00:04, 35.35it/s]Evaluating:  37%|███▋      | 92/250 [00:02<00:04, 35.50it/s]Evaluating:  38%|███▊      | 96/250 [00:03<00:04, 35.57it/s]Evaluating:  40%|████      | 100/250 [00:03<00:04, 35.64it/s]Evaluating:  42%|████▏     | 104/250 [00:03<00:04, 35.59it/s]Evaluating:  43%|████▎     | 108/250 [00:03<00:03, 35.75it/s]Evaluating:  45%|████▍     | 112/250 [00:03<00:03, 35.88it/s]Evaluating:  46%|████▋     | 116/250 [00:03<00:03, 35.97it/s]Evaluating:  48%|████▊     | 120/250 [00:03<00:03, 35.74it/s]Evaluating:  50%|████▉     | 124/250 [00:03<00:03, 35.75it/s]Evaluating:  51%|█████     | 128/250 [00:03<00:03, 35.84it/s]Evaluating:  53%|█████▎    | 132/250 [00:04<00:03, 35.88it/s]Evaluating:  54%|█████▍    | 136/250 [00:04<00:03, 35.57it/s]Evaluating:  56%|█████▌    | 140/250 [00:04<00:03, 35.77it/s]Evaluating:  58%|█████▊    | 144/250 [00:04<00:02, 35.85it/s]Evaluating:  59%|█████▉    | 148/250 [00:04<00:02, 35.77it/s]Evaluating:  61%|██████    | 152/250 [00:04<00:02, 35.80it/s]Evaluating:  62%|██████▏   | 156/250 [00:04<00:02, 35.57it/s]Evaluating:  64%|██████▍   | 160/250 [00:04<00:02, 35.79it/s]Evaluating:  66%|██████▌   | 164/250 [00:04<00:02, 35.79it/s]Evaluating:  67%|██████▋   | 168/250 [00:05<00:02, 35.95it/s]Evaluating:  69%|██████▉   | 172/250 [00:05<00:02, 35.60it/s]Evaluating:  70%|███████   | 176/250 [00:05<00:02, 35.75it/s]Evaluating:  72%|███████▏  | 180/250 [00:05<00:01, 35.73it/s]Evaluating:  74%|███████▎  | 184/250 [00:05<00:01, 35.75it/s]Evaluating:  75%|███████▌  | 188/250 [00:05<00:01, 35.71it/s]Evaluating:  77%|███████▋  | 192/250 [00:05<00:01, 35.44it/s]Evaluating:  78%|███████▊  | 196/250 [00:05<00:01, 35.58it/s]Evaluating:  80%|████████  | 200/250 [00:05<00:01, 35.92it/s]Evaluating:  82%|████████▏ | 204/250 [00:06<00:01, 35.90it/s]Evaluating:  83%|████████▎ | 208/250 [00:06<00:01, 35.38it/s]Evaluating:  85%|████████▍ | 212/250 [00:06<00:01, 35.61it/s]Evaluating:  86%|████████▋ | 216/250 [00:06<00:00, 35.81it/s]Evaluating:  88%|████████▊ | 220/250 [00:06<00:00, 35.40it/s]Evaluating:  90%|████████▉ | 224/250 [00:06<00:00, 35.70it/s]Evaluating:  91%|█████████ | 228/250 [00:06<00:00, 35.25it/s]Evaluating:  93%|█████████▎| 232/250 [00:06<00:00, 35.38it/s]Evaluating:  94%|█████████▍| 236/250 [00:06<00:00, 35.43it/s]Evaluating:  96%|█████████▌| 240/250 [00:07<00:00, 35.59it/s]Evaluating:  98%|█████████▊| 244/250 [00:07<00:00, 35.88it/s]Evaluating:  99%|█████████▉| 248/250 [00:07<00:00, 36.02it/s]Evaluating: 100%|██████████| 250/250 [00:07<00:00, 34.07it/s]
04/13/2020 14:15:24 - INFO - __main__ -   preds: 
04/13/2020 14:15:24 - INFO - __main__ -   [1 0 1 ... 1 0 0]
04/13/2020 14:15:24 - INFO - __main__ -   labels: 
04/13/2020 14:15:24 - INFO - __main__ -   [1 0 1 ... 0 1 0]
04/13/2020 14:15:24 - INFO - __main__ -   ***** Eval results  *****
04/13/2020 14:15:24 - INFO - __main__ -     acc = 0.75
Traceback (most recent call last):
  File "/scratch/fs45/nlu/code/transformers/examples/run_glue.py", line 708, in <module>
    main()
  File "/scratch/fs45/nlu/code/transformers/examples/run_glue.py", line 700, in main
    result = evaluate(args, model, tokenizer, prefix=prefix)
  File "/scratch/fs45/nlu/code/transformers/examples/run_glue.py", line 368, in evaluate
    writer.write(preds)
TypeError: write() argument must be str, not numpy.ndarray
