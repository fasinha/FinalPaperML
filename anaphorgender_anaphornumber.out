04/13/2020 14:01:42 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
04/13/2020 14:01:43 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/fs45/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
04/13/2020 14:01:43 - INFO - transformers.configuration_utils -   Model config BertConfig {
  "_num_labels": 2,
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": null,
  "do_sample": false,
  "early_stopping": false,
  "eos_token_ids": null,
  "finetuning_task": "blimptask",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_beams": 1,
  "num_hidden_layers": 12,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": null,
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

04/13/2020 14:01:43 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/fs45/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
04/13/2020 14:01:43 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/fs45/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
04/13/2020 14:01:49 - INFO - transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
04/13/2020 14:01:49 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
04/13/2020 14:01:54 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='/scratch/fs45/nlu/data/blimp/data//blimptask', device=device(type='cuda'), do_eval=True, do_lower_case=False, do_train=False, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=2e-05, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='bert-base-uncased', model_type='bert', n_gpu=1, no_cuda=False, num_train_epochs=3.0, output_dir='/scratch/fs45/nlu/blimptask_run/', output_mode='classification', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, save_steps=500, seed=42, server_ip='', server_port='', task_name='blimptask', tokenizer_name='', warmup_steps=0, weight_decay=0.0)
04/13/2020 14:01:54 - INFO - transformers.tokenization_utils -   Model name '/scratch/fs45/nlu/blimptask_run/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '/scratch/fs45/nlu/blimptask_run/' is a path, a model identifier, or url to a directory containing tokenizer files.
04/13/2020 14:01:54 - INFO - transformers.tokenization_utils -   Didn't find file /scratch/fs45/nlu/blimptask_run/added_tokens.json. We won't load it.
04/13/2020 14:01:54 - INFO - transformers.tokenization_utils -   loading file /scratch/fs45/nlu/blimptask_run/vocab.txt
04/13/2020 14:01:54 - INFO - transformers.tokenization_utils -   loading file None
04/13/2020 14:01:54 - INFO - transformers.tokenization_utils -   loading file /scratch/fs45/nlu/blimptask_run/special_tokens_map.json
04/13/2020 14:01:54 - INFO - transformers.tokenization_utils -   loading file /scratch/fs45/nlu/blimptask_run/tokenizer_config.json
04/13/2020 14:01:54 - INFO - __main__ -   Evaluate the following checkpoints: ['/scratch/fs45/nlu/blimptask_run/']
04/13/2020 14:01:54 - INFO - transformers.configuration_utils -   loading configuration file /scratch/fs45/nlu/blimptask_run/config.json
04/13/2020 14:01:54 - INFO - transformers.configuration_utils -   Model config BertConfig {
  "_num_labels": 2,
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": null,
  "do_sample": false,
  "early_stopping": false,
  "eos_token_ids": null,
  "finetuning_task": "blimptask",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_beams": 1,
  "num_hidden_layers": 12,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": null,
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

04/13/2020 14:01:54 - INFO - transformers.modeling_utils -   loading weights file /scratch/fs45/nlu/blimptask_run/pytorch_model.bin
04/13/2020 14:01:57 - INFO - __main__ -   Loading features from cached file /scratch/fs45/nlu/data/blimp/data//blimptask/cached_dev_bert-base-uncased_128_blimptask
04/13/2020 14:01:57 - INFO - __main__ -   ***** Running evaluation  *****
04/13/2020 14:01:57 - INFO - __main__ -     Num examples = 2000
04/13/2020 14:01:57 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/250 [00:00<?, ?it/s]Evaluating:   0%|          | 1/250 [00:00<03:28,  1.20it/s]Evaluating:   2%|▏         | 4/250 [00:00<02:26,  1.68it/s]Evaluating:   3%|▎         | 7/250 [00:01<01:43,  2.34it/s]Evaluating:   4%|▍         | 10/250 [00:01<01:14,  3.23it/s]Evaluating:   5%|▌         | 13/250 [00:01<00:53,  4.40it/s]Evaluating:   6%|▋         | 16/250 [00:01<00:39,  5.91it/s]Evaluating:   8%|▊         | 19/250 [00:01<00:29,  7.78it/s]Evaluating:   9%|▉         | 22/250 [00:01<00:22, 10.00it/s]Evaluating:  10%|█         | 25/250 [00:01<00:18, 12.48it/s]Evaluating:  11%|█         | 28/250 [00:01<00:14, 15.11it/s]Evaluating:  12%|█▏        | 31/250 [00:01<00:12, 17.72it/s]Evaluating:  14%|█▎        | 34/250 [00:01<00:10, 20.13it/s]Evaluating:  15%|█▍        | 37/250 [00:02<00:09, 22.28it/s]Evaluating:  16%|█▌        | 40/250 [00:02<00:08, 24.08it/s]Evaluating:  17%|█▋        | 43/250 [00:02<00:08, 25.53it/s]Evaluating:  18%|█▊        | 46/250 [00:02<00:07, 26.65it/s]Evaluating:  20%|█▉        | 49/250 [00:02<00:07, 27.50it/s]Evaluating:  21%|██        | 52/250 [00:02<00:07, 28.13it/s]Evaluating:  22%|██▏       | 55/250 [00:02<00:06, 28.58it/s]Evaluating:  23%|██▎       | 58/250 [00:02<00:06, 28.91it/s]Evaluating:  24%|██▍       | 61/250 [00:02<00:06, 29.09it/s]Evaluating:  26%|██▌       | 64/250 [00:02<00:06, 29.27it/s]Evaluating:  27%|██▋       | 67/250 [00:03<00:06, 29.40it/s]Evaluating:  28%|██▊       | 70/250 [00:03<00:06, 29.49it/s]Evaluating:  29%|██▉       | 73/250 [00:03<00:05, 29.56it/s]Evaluating:  30%|███       | 76/250 [00:03<00:05, 29.60it/s]Evaluating:  32%|███▏      | 79/250 [00:03<00:05, 29.63it/s]Evaluating:  33%|███▎      | 82/250 [00:03<00:05, 29.56it/s]Evaluating:  34%|███▍      | 85/250 [00:03<00:05, 29.56it/s]Evaluating:  35%|███▌      | 88/250 [00:03<00:05, 29.60it/s]Evaluating:  36%|███▋      | 91/250 [00:03<00:05, 29.54it/s]Evaluating:  38%|███▊      | 94/250 [00:03<00:05, 29.58it/s]Evaluating:  39%|███▉      | 97/250 [00:04<00:05, 29.61it/s]Evaluating:  40%|████      | 100/250 [00:04<00:05, 29.63it/s]Evaluating:  41%|████      | 103/250 [00:04<00:04, 29.65it/s]Evaluating:  42%|████▏     | 106/250 [00:04<00:04, 29.66it/s]Evaluating:  44%|████▎     | 109/250 [00:04<00:04, 29.68it/s]Evaluating:  45%|████▍     | 112/250 [00:04<00:04, 29.68it/s]Evaluating:  46%|████▌     | 115/250 [00:04<00:04, 29.69it/s]Evaluating:  47%|████▋     | 118/250 [00:04<00:04, 29.70it/s]Evaluating:  48%|████▊     | 121/250 [00:04<00:04, 29.64it/s]Evaluating:  50%|████▉     | 124/250 [00:05<00:04, 29.66it/s]Evaluating:  51%|█████     | 127/250 [00:05<00:04, 29.67it/s]Evaluating:  52%|█████▏    | 130/250 [00:05<00:04, 29.67it/s]Evaluating:  53%|█████▎    | 133/250 [00:05<00:03, 29.68it/s]Evaluating:  54%|█████▍    | 136/250 [00:05<00:03, 29.68it/s]Evaluating:  56%|█████▌    | 139/250 [00:05<00:03, 29.69it/s]Evaluating:  57%|█████▋    | 142/250 [00:05<00:03, 29.69it/s]Evaluating:  58%|█████▊    | 145/250 [00:05<00:03, 29.70it/s]Evaluating:  59%|█████▉    | 148/250 [00:05<00:03, 29.70it/s]Evaluating:  60%|██████    | 151/250 [00:05<00:03, 29.65it/s]Evaluating:  62%|██████▏   | 154/250 [00:06<00:03, 29.67it/s]Evaluating:  63%|██████▎   | 157/250 [00:06<00:03, 29.67it/s]Evaluating:  64%|██████▍   | 160/250 [00:06<00:03, 29.68it/s]Evaluating:  65%|██████▌   | 163/250 [00:06<00:02, 29.68it/s]Evaluating:  66%|██████▋   | 166/250 [00:06<00:02, 29.69it/s]Evaluating:  68%|██████▊   | 169/250 [00:06<00:02, 29.70it/s]Evaluating:  69%|██████▉   | 172/250 [00:06<00:02, 29.70it/s]Evaluating:  70%|███████   | 175/250 [00:06<00:02, 29.71it/s]Evaluating:  71%|███████   | 178/250 [00:06<00:02, 29.71it/s]Evaluating:  72%|███████▏  | 181/250 [00:06<00:02, 29.64it/s]Evaluating:  74%|███████▎  | 184/250 [00:07<00:02, 29.66it/s]Evaluating:  75%|███████▍  | 187/250 [00:07<00:02, 29.68it/s]Evaluating:  76%|███████▌  | 190/250 [00:07<00:02, 29.68it/s]Evaluating:  77%|███████▋  | 193/250 [00:07<00:01, 29.69it/s]Evaluating:  78%|███████▊  | 196/250 [00:07<00:01, 29.69it/s]Evaluating:  80%|███████▉  | 199/250 [00:07<00:01, 29.69it/s]Evaluating:  81%|████████  | 202/250 [00:07<00:01, 29.70it/s]Evaluating:  82%|████████▏ | 205/250 [00:07<00:01, 29.70it/s]Evaluating:  83%|████████▎ | 208/250 [00:07<00:01, 29.70it/s]Evaluating:  84%|████████▍ | 211/250 [00:07<00:01, 29.64it/s]Evaluating:  86%|████████▌ | 214/250 [00:08<00:01, 29.65it/s]Evaluating:  87%|████████▋ | 217/250 [00:08<00:01, 29.67it/s]Evaluating:  88%|████████▊ | 220/250 [00:08<00:01, 29.67it/s]Evaluating:  89%|████████▉ | 223/250 [00:08<00:00, 29.69it/s]Evaluating:  90%|█████████ | 226/250 [00:08<00:00, 29.69it/s]Evaluating:  92%|█████████▏| 229/250 [00:08<00:00, 29.69it/s]Evaluating:  93%|█████████▎| 232/250 [00:08<00:00, 29.69it/s]Evaluating:  94%|█████████▍| 235/250 [00:08<00:00, 29.70it/s]Evaluating:  95%|█████████▌| 238/250 [00:08<00:00, 29.70it/s]Evaluating:  96%|█████████▋| 241/250 [00:08<00:00, 29.63it/s]Evaluating:  98%|█████████▊| 244/250 [00:09<00:00, 29.64it/s]Evaluating:  99%|█████████▉| 247/250 [00:09<00:00, 29.66it/s]Evaluating: 100%|██████████| 250/250 [00:09<00:00, 29.67it/s]Evaluating: 100%|██████████| 250/250 [00:09<00:00, 27.04it/s]
04/13/2020 14:02:06 - INFO - __main__ -   preds: 
04/13/2020 14:02:06 - INFO - __main__ -   [1 0 1 ... 1 0 0]
04/13/2020 14:02:06 - INFO - __main__ -   labels: 
04/13/2020 14:02:06 - INFO - __main__ -   [1 0 1 ... 0 1 0]
04/13/2020 14:02:06 - INFO - __main__ -   ***** Eval results  *****
04/13/2020 14:02:06 - INFO - __main__ -     acc = 0.75
Traceback (most recent call last):
  File "/scratch/fs45/nlu/code/transformers/examples/run_glue.py", line 708, in <module>
    main()
  File "/scratch/fs45/nlu/code/transformers/examples/run_glue.py", line 700, in main
    result = evaluate(args, model, tokenizer, prefix=prefix)
  File "/scratch/fs45/nlu/code/transformers/examples/run_glue.py", line 368, in evaluate
    writer.write(preds)
TypeError: write() argument must be str, not numpy.ndarray
